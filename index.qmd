---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**

## Python Code

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

### Bivariate Regression: Anxiety on StressSurvey

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on StressSurvey
X = observDF['StressSurvey'].values
y = observDF['Anxiety'].values

# Calculate regression coefficients manually
n = len(X)
sum_x = np.sum(X)
sum_y = np.sum(y)
sum_xy = np.sum(X * y)
sum_x2 = np.sum(X * X)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept = (sum_y - slope * sum_x) / n

# Calculate R-squared
y_pred = intercept + slope * X
ss_res = np.sum((y - y_pred) ** 2)
ss_tot = np.sum((y - np.mean(y)) ** 2)
r_squared = 1 - (ss_res / ss_tot)

print("Regression Results:")
print(f"Intercept (beta0): {intercept:.4f}")
print(f"Slope (beta1): {slope:.4f}")
print(f"R-squared: {r_squared:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"Estimated coefficient of StressSurvey: {slope:.4f}")
print(f"Difference from true: {abs(slope - 1.0):.4f}")
```

### Question 1: What are the estimated coefficients? How do they compare to the true coefficients

The estimated coefficients are 1.0470 for the StressSurvey (slope) and -1.5240 for the intercept. The true coefficients are 1.0 for the stress, 0 for the intercept, and 0.1 for the Time. The difference from the true coefficients is 0.0470 for StressSurvey and -1.5240 for the intercept. This tells me that the StressSurvey is a good proxy for the true Stress variable, but the intercept is not a good proxy for the true intercept as it is not 0.

### Scatter Plot with Regression Line

```{python}
#| echo: true
# Create scatter plot with regression line
plt.figure(figsize=(10, 6))
plt.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, s=100, color='blue', label='Data Points')

# Plot regression line
x_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred_line = intercept + slope * x_range
plt.plot(x_range, y_pred_line, 'r-', linewidth=2, label=f'Regression Line (y = {intercept:.3f} + {slope:.3f}x)')

plt.xlabel('StressSurvey', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between StressSurvey and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis
print("Fit Analysis:")
print(f"R-squared: {r_squared:.4f} ({r_squared*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res/n):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y - y_pred)):.4f}")

# Check for potential issues
print("\nPotential Issues Analysis:")
print("1. Clustering: Notice the data points cluster at specific StressSurvey values (0, 3, 6, 9, 12)")
print("2. Limited variability: Most variation occurs at discrete levels rather than continuous")
print("3. Perfect linear pattern: The relationship appears artificially perfect")
```

### Question 2: Commentary on Fit and Potential Issues

**Model Fit Assessment:**

The fit is pretty good with a R-squared of 90.11%. The root mean square error is 0.7828 and the mean absolute error is 0.6241. This tells me that the model is a good fit for the data. The data points are scattered around the regression line and the residuals are not clustered at any specific time value. This is a good sign as it suggests that the model is a good fit for the data.

---

### Bivariate Regression: Anxiety on Time

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on Time
X_time = observDF['Time'].values
y_time = observDF['Anxiety'].values

# Calculate regression coefficients manually
n_time = len(X_time)
sum_x_time = np.sum(X_time)
sum_y_time = np.sum(y_time)
sum_xy_time = np.sum(X_time * y_time)
sum_x2_time = np.sum(X_time * X_time)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope_time = (n_time * sum_xy_time - sum_x_time * sum_y_time) / (n_time * sum_x2_time - sum_x_time * sum_x_time)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept_time = (sum_y_time - slope_time * sum_x_time) / n_time

# Calculate R-squared
y_pred_time = intercept_time + slope_time * X_time
ss_res_time = np.sum((y_time - y_pred_time) ** 2)
ss_tot_time = np.sum((y_time - np.mean(y_time)) ** 2)
r_squared_time = 1 - (ss_res_time / ss_tot_time)

print("Regression Results (Anxiety on Time):")
print(f"Intercept (beta0): {intercept_time:.4f}")
print(f"Slope (beta1): {slope_time:.4f}")
print(f"R-squared: {r_squared_time:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Time: 0.1")
print(f"Estimated coefficient of Time: {slope_time:.4f}")
print(f"Difference from true: {abs(slope_time - 0.1):.4f}")

# Create scatter plot for Time vs Anxiety
plt.figure(figsize=(10, 6))
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', label='Data Points')

# Plot regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_line_time = intercept_time + slope_time * x_range_time
plt.plot(x_range_time, y_pred_line_time, 'r-', linewidth=2, label=f'Regression Line (y = {intercept_time:.3f} + {slope_time:.3f}x)')

plt.xlabel('Time', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between Time and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_time/n_time):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_time - y_pred_time)):.4f}")
```

### Question 3: What are the estimated coefficients? How do they compare to the true relationship?

The estimated coefficients are 5.3406 for the time and -3.6801 for the intercept. The true coefficients are 0.1 for the time and 0 for the intercept. The difference from the true coefficients is 5.2406 for the time and -3.6801 for the intercept. The time coefficient is much larger than the true value, which means that the model overestimates the effect of time on anxiety.

### Scatter Plot with Regression Line: Time vs Anxiety

```{python}
#| echo: true
# Create scatter plot with regression line for Time vs Anxiety
plt.figure(figsize=(10, 6))
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', label='Data Points')

# Plot regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_line_time = intercept_time + slope_time * x_range_time
plt.plot(x_range_time, y_pred_line_time, 'r-', linewidth=2, label=f'Regression Line (y = {intercept_time:.3f} + {slope_time:.3f}x)')

plt.xlabel('Time', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between Time and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis
print("Fit Analysis:")
print(f"R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_time/n_time):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_time - y_pred_time)):.4f}")

# Check for potential issues
print("\nPotential Issues Analysis:")
print("1. Clustering: Notice the data points cluster at specific Time values (0, 1, 2, 2.1, 2.2)")
print("2. Limited variability: Most variation occurs at discrete levels rather than continuous")
print("3. Poor fit: R-squared of 56.3% indicates Time alone is not a strong predictor")
print("4. Large residuals: Substantial prediction errors throughout the range")
```

### Question 4: Commentary on Fit and Potential Issues

The fit is not very good with a R-squared of 56.3%. The root mean square error is 3.0933 and the mean absolute error is 2.5607. This tells me that the model is not a good fit for the data. The data points are clustered at specific Time values (0, 1, 2, 2.1, 2.2) and the residuals are not random. This is a bad sign as it suggests that the model is not a good fit for the data. Time by itself is not a good predictor of anxiety.

---

### Multiple Regression: Anxiety on StressSurvey and Time

```{python}
#| echo: true
# Multiple regression of Anxiety on both StressSurvey and Time
# Using matrix algebra for multiple regression: β = (X'X)^(-1)X'y

# Create design matrix X with intercept, StressSurvey, and Time
X_multi = np.column_stack([
    np.ones(len(observDF)),  # Intercept column
    observDF['StressSurvey'].values,  # StressSurvey column
    observDF['Time'].values  # Time column
])

y_multi = observDF['Anxiety'].values

# Calculate coefficients using normal equation: β = (X'X)^(-1)X'y
X_transpose = X_multi.T
X_transpose_X = np.dot(X_transpose, X_multi)
X_transpose_X_inv = np.linalg.inv(X_transpose_X)
X_transpose_y = np.dot(X_transpose, y_multi)
coefficients = np.dot(X_transpose_X_inv, X_transpose_y)

# Extract coefficients
intercept_multi = coefficients[0]
slope_stress_survey = coefficients[1]
slope_time = coefficients[2]

# Calculate predictions and R-squared
y_pred_multi = np.dot(X_multi, coefficients)
ss_res_multi = np.sum((y_multi - y_pred_multi) ** 2)
ss_tot_multi = np.sum((y_multi - np.mean(y_multi)) ** 2)
r_squared_multi = 1 - (ss_res_multi / ss_tot_multi)

print("Multiple Regression Results:")
print(f"Intercept (β₀): {intercept_multi:.4f}")
print(f"StressSurvey coefficient (β₁): {slope_stress_survey:.4f}")
print(f"Time coefficient (β₂): {slope_time:.4f}")
print(f"R-squared: {r_squared_multi:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"True coefficient of Time: 0.1")
print()
print(f"Estimated StressSurvey coefficient: {slope_stress_survey:.4f}")
print(f"Difference from true Stress: {abs(slope_stress_survey - 1.0):.4f}")
print()
print(f"Estimated Time coefficient: {slope_time:.4f}")
print(f"Difference from true Time: {abs(slope_time - 0.1):.4f}")

# Additional fit statistics
print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_multi:.4f} ({r_squared_multi*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_multi/len(y_multi)):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_multi - y_pred_multi)):.4f}")

# Create 3D scatter plot
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Time'], observDF['Anxiety'], 
           c='blue', alpha=0.7, s=100, label='Data Points')

# Create mesh for regression plane
stress_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 10)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 10)
Stress_mesh, Time_mesh = np.meshgrid(stress_range, time_range)
Anxiety_pred = intercept_multi + slope_stress_survey * Stress_mesh + slope_time * Time_mesh

# Plot regression plane
ax.plot_surface(Stress_mesh, Time_mesh, Anxiety_pred, alpha=0.3, color='red', 
                label='Regression Plane')

ax.set_xlabel('StressSurvey')
ax.set_ylabel('Time')
ax.set_zlabel('Anxiety')
ax.set_title('Multiple Regression: Anxiety = f(StressSurvey, Time)')
plt.tight_layout()
plt.show()
```

### Question 5: What are the estimated coefficients? How do they compare to the true relationship?

The estimated coefficients are 0.588 for the intercept, 1.4269 for StressSurvey, and -2.7799. The true coefficients are 0 for the intercept, 1 for the stress, and .1 for the time. Time is concerning as it is not close to the true value. The time coefficient suggest that for every 1 unit of time, the anxiety decreases by 2.7799 units. This is quite different from the true relationship. 

---

### Multiple Regression: Anxiety on TRUE Stress and Time

```{python}
#| echo: true
# Multiple regression of Anxiety on both TRUE Stress and Time
# Using matrix algebra for multiple regression: β = (X'X)^(-1)X'y

# Create design matrix X with intercept, TRUE Stress, and Time
X_multi_true = np.column_stack([
    np.ones(len(observDF)),  # Intercept column
    observDF['Stress'].values,  # TRUE Stress column (not StressSurvey)
    observDF['Time'].values  # Time column
])

y_multi_true = observDF['Anxiety'].values

# Calculate coefficients using normal equation: β = (X'X)^(-1)X'y
X_transpose_true = X_multi_true.T
X_transpose_X_true = np.dot(X_transpose_true, X_multi_true)
X_transpose_X_inv_true = np.linalg.inv(X_transpose_X_true)
X_transpose_y_true = np.dot(X_transpose_true, y_multi_true)
coefficients_true = np.dot(X_transpose_X_inv_true, X_transpose_y_true)

# Extract coefficients
intercept_multi_true = coefficients_true[0]
slope_stress_true = coefficients_true[1]
slope_time_true = coefficients_true[2]

# Calculate predictions and R-squared
y_pred_multi_true = np.dot(X_multi_true, coefficients_true)
ss_res_multi_true = np.sum((y_multi_true - y_pred_multi_true) ** 2)
ss_tot_multi_true = np.sum((y_multi_true - np.mean(y_multi_true)) ** 2)
r_squared_multi_true = 1 - (ss_res_multi_true / ss_tot_multi_true)

print("Multiple Regression Results (TRUE Variables):")
print(f"Intercept (β₀): {intercept_multi_true:.4f}")
print(f"Stress coefficient (β₁): {slope_stress_true:.4f}")
print(f"Time coefficient (β₂): {slope_time_true:.4f}")
print(f"R-squared: {r_squared_multi_true:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"True coefficient of Time: 0.1")
print(f"True intercept: 0.0")
print()
print(f"Estimated Stress coefficient: {slope_stress_true:.4f}")
print(f"Difference from true Stress: {abs(slope_stress_true - 1.0):.4f}")
print()
print(f"Estimated Time coefficient: {slope_time_true:.4f}")
print(f"Difference from true Time: {abs(slope_time_true - 0.1):.4f}")
print()
print(f"Estimated intercept: {intercept_multi_true:.4f}")
print(f"Difference from true intercept: {abs(intercept_multi_true - 0.0):.4f}")

# Additional fit statistics
print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_multi_true:.4f} ({r_squared_multi_true*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_multi_true/len(y_multi_true)):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_multi_true - y_pred_multi_true)):.4f}")

# Create 3D scatter plot with TRUE variables
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
ax.scatter(observDF['Stress'], observDF['Time'], observDF['Anxiety'], 
           c='blue', alpha=0.7, s=100, label='Data Points')

# Create mesh for regression plane
stress_range_true = np.linspace(observDF['Stress'].min(), observDF['Stress'].max(), 10)
time_range_true = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 10)
Stress_mesh_true, Time_mesh_true = np.meshgrid(stress_range_true, time_range_true)
Anxiety_pred_true = intercept_multi_true + slope_stress_true * Stress_mesh_true + slope_time_true * Time_mesh_true

# Plot regression plane
ax.plot_surface(Stress_mesh_true, Time_mesh_true, Anxiety_pred_true, alpha=0.3, color='red', 
                label='Regression Plane')

ax.set_xlabel('Stress (TRUE)')
ax.set_ylabel('Time')
ax.set_zlabel('Anxiety')
ax.set_title('Multiple Regression: Anxiety = f(Stress, Time)\nPerfect Fit (R² = 1.000)')
plt.tight_layout()
plt.show()
```

### Questions to answer for 85% Grade on Challenge:

### Question 4: What are the estimated coefficients? How do they compare to the true relationship?

The estimated coefficients are 0 for the intercept, 1 for the stress, and .1 for the time. These are the same as the true coefficients. The true relationship is Anxiety = Stress + 0.1 × Time. These estimateed coefficents demonstrate that the model is a perfect fit for the data. Using true stress rather than stress survey results in a perfect fit.


---

## Comparison of Multiple Regression Models with Statistical Significance Testing

```{python}
#| echo: true
# Compare both multiple regression models with full statistical output
import statsmodels.api as sm

# Model 1: Anxiety ~ StressSurvey + Time
X_model1 = sm.add_constant(observDF[['StressSurvey', 'Time']])
y_model1 = observDF['Anxiety']
model1 = sm.OLS(y_model1, X_model1).fit()

print("="*80)
print("MODEL 1: Anxiety ~ StressSurvey + Time")
print("="*80)
print(model1.summary())
print("\n")

# Model 2: Anxiety ~ TRUE Stress + Time
X_model2 = sm.add_constant(observDF[['Stress', 'Time']])
y_model2 = observDF['Anxiety']
model2 = sm.OLS(y_model2, X_model2).fit()

print("="*80)
print("MODEL 2: Anxiety ~ TRUE Stress + Time")
print("="*80)
print(model2.summary())
```

### Detailed Comparison Analysis

```{python}
#| echo: true
# Create comparison table
import pandas as pd

comparison_data = {
    'Metric': [
        'R-squared',
        'Adjusted R-squared',
        'Intercept Coefficient',
        'Intercept p-value',
        'Intercept Significant?',
        'Stress Coefficient',
        'Stress p-value',
        'Stress Significant?',
        'Time Coefficient',
        'Time p-value',
        'Time Significant?',
        'RMSE',
        'F-statistic',
        'F-statistic p-value'
    ],
    'Model 1 (StressSurvey)': [
        f"{model1.rsquared:.6f}",
        f"{model1.rsquared_adj:.6f}",
        f"{model1.params['const']:.6f}",
        f"{model1.pvalues['const']:.6f}",
        'Yes' if model1.pvalues['const'] < 0.05 else 'No',
        f"{model1.params['StressSurvey']:.6f}",
        f"{model1.pvalues['StressSurvey']:.6f}",
        'Yes' if model1.pvalues['StressSurvey'] < 0.05 else 'No',
        f"{model1.params['Time']:.6f}",
        f"{model1.pvalues['Time']:.6f}",
        'Yes' if model1.pvalues['Time'] < 0.05 else 'No',
        f"{np.sqrt(model1.mse_resid):.6f}",
        f"{model1.fvalue:.2f}",
        f"{model1.f_pvalue:.6f}"
    ],
    'Model 2 (TRUE Stress)': [
        f"{model2.rsquared:.6f}",
        f"{model2.rsquared_adj:.6f}",
        f"{model2.params['const']:.6f}",
        f"{model2.pvalues['const']:.10f}" if model2.pvalues['const'] < 0.001 else f"{model2.pvalues['const']:.6f}",
        'Yes' if model2.pvalues['const'] < 0.05 else 'No',
        f"{model2.params['Stress']:.6f}",
        f"{model2.pvalues['Stress']:.10f}" if model2.pvalues['Stress'] < 0.001 else f"{model2.pvalues['Stress']:.6f}",
        'Yes' if model2.pvalues['Stress'] < 0.05 else 'No',
        f"{model2.params['Time']:.6f}",
        f"{model2.pvalues['Time']:.10f}" if model2.pvalues['Time'] < 0.001 else f"{model2.pvalues['Time']:.6f}",
        'Yes' if model2.pvalues['Time'] < 0.05 else 'No',
        f"{np.sqrt(model2.mse_resid):.6f}",
        f"{model2.fvalue:.2f}",
        f"{model2.f_pvalue:.10f}" if model2.f_pvalue < 0.001 else f"{model2.f_pvalue:.6f}"
    ],
    'True Values': [
        '1.0000',
        '1.0000',
        '0.0000',
        'N/A',
        'N/A',
        '1.0000',
        'N/A',
        'N/A',
        '0.1000',
        'N/A',
        'N/A',
        '0.0000',
        'N/A',
        'N/A'
    ]
}

comparison_df = pd.DataFrame(comparison_data)
print("\n" + "="*100)
print("COMPREHENSIVE MODEL COMPARISON")
print("="*100)
print(comparison_df.to_string(index=False))
print("="*100)

# Additional insights
print("\n" + "="*80)
print("KEY INSIGHTS FROM COMPARISON")
print("="*80)

print("\n1. R-SQUARED COMPARISON:")
print(f"   Model 1 (StressSurvey): {model1.rsquared:.6f} ({model1.rsquared*100:.2f}% variance explained)")
print(f"   Model 2 (TRUE Stress):  {model2.rsquared:.10f} ({model2.rsquared*100:.2f}% variance explained)")
print(f"   → Model 2 achieves essentially perfect fit (R² ≈ 1.0000)")

print("\n2. COEFFICIENT INTERPRETATIONS:")
print("\n   MODEL 1 (StressSurvey + Time):")
print(f"   • Intercept: {model1.params['const']:.6f}")
print(f"     → For every 1-unit increase in StressSurvey, Anxiety increases by {model1.params['StressSurvey']:.6f}")
print(f"   • StressSurvey: {model1.params['StressSurvey']:.6f}")
print(f"     → Very close to true value of 1.0 (difference: {abs(model1.params['StressSurvey'] - 1.0):.6f})")
print(f"   • Time: {model1.params['Time']:.6f}")
print(f"     → Close to true value of 0.1 (difference: {abs(model1.params['Time'] - 0.1):.6f})")

print("\n   MODEL 2 (TRUE Stress + Time):")
print(f"   • Intercept: {model2.params['const']:.10f}")
print(f"     → Essentially 0, matching true value perfectly")
print(f"   • Stress: {model2.params['Stress']:.10f}")
print(f"     → Essentially 1.0, matching true value perfectly")
print(f"   • Time: {model2.params['Time']:.10f}")
print(f"     → Essentially 0.1, matching true value perfectly")

print("\n3. STATISTICAL SIGNIFICANCE (α = 0.05):")
print("\n   MODEL 1:")
print(f"   • Intercept: p = {model1.pvalues['const']:.6f} → {'SIGNIFICANT' if model1.pvalues['const'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   • StressSurvey: p = {model1.pvalues['StressSurvey']:.6f} → {'SIGNIFICANT' if model1.pvalues['StressSurvey'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   • Time: p = {model1.pvalues['Time']:.6f} → {'SIGNIFICANT' if model1.pvalues['Time'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   → All coefficients significant? {all(model1.pvalues < 0.05)}")

print("\n   MODEL 2:")
print(f"   • Intercept: p = {model2.pvalues['const']:.10f} → {'SIGNIFICANT' if model2.pvalues['const'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   • Stress: p = {model2.pvalues['Stress']:.10f} → {'SIGNIFICANT' if model2.pvalues['Stress'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   • Time: p = {model2.pvalues['Time']:.10f} → {'SIGNIFICANT' if model2.pvalues['Time'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"   → All coefficients significant? {all(model2.pvalues < 0.05)}")
```

### Question 5: Compare R-squared values and coefficient interpretations between the two models. Do both models show statistical significance in all coefficient estimates? What does this tell you about real-world implications of multiple regression results?

**R-squared Comparison:**

Model 1 (StressSurvey + Time) achieves an R² of approximately 0.935, explaining 93.5% of the variance in Anxiety. Model 2 (TRUE Stress + Time) achieves an R² of 1.0000. The minimal difference between them shows that StressSurvey is a good proxy for the true Stress variable.

**Coefficient Interpretations:**

The model involving StressSurvey and Time had a coefficient of 1.4269 for the StressSurvey and -2.7799 for the time. The model involving true stress and time had a coefficient of 1 for the stress and 0.1 for the time. The model involving true stress and time is a much better fit. In model 1,StressSurvey overpredicts the true anxiety level and time underpredicts the true anxiety level. Model 2 is much more accurate because it uses the true Stress variable rather than the survey proxy.

**Looking at the p-values from the statistical tests:**

**Model 1:** The results show that the StressSurvey and Time coefficients are highly statistically significant (p < 0.001), but the intercept's significance depends on the specific p-value calculated. When the intercept is very close to 0, it may not be statistically different from 0.

**Model 2:** All coefficients (Stress and Time) are extremely statistically significant (p ≈ 0), indicating perfect estimation of the true relationship. The intercept, being essentially 0, may not be statistically significant, which actually makes sense—it's not significantly different from its true value of 0.


 Time spent on social media is not a good predictor of anxiety levels by itself since it had a R-squared of 56.3% and a coefficient of 5.3406. Once we ran multiple regressions with both stress and StressSurvey, the model become much more accurate.  The real-world implications learned from these results is to be careful with adding in variables in multiple regression since it can lead to overfitting and misleading conclusions. 



### Question 6: Reflect on Real-World Implications
For the first model, a headline could be "Time spent on Social Media Reduces Anxiety". This model shows that the more time spent on social media, the lower the anxiety level as the coefficient is -2.7799. However, the model is not capturing the true relationship as StressSurvey is not the true Stress variable and is throwing off the results. For Model 2, a headline could be "Time spent on Social Media increases Anxiety". This model shows that the more time spent on social media, the higher the anxiety level as the coefficient for time is 0.1. A typical parent will believe the second headline as it plays into the parent's belief that social media is harmful to their children. As for Facebook, Instagram, and TikTok executives, they will likely believe the first headline as it plays into their belief that social media is beneficial to their users and can actually reduce anxiety.



### Question 7: Avoiding Misleading Statistical Significance: 

---

## Sample Splitting Analysis: High Stress Regime

### Data Exploration: Identifying Statistical Regimes

```{python}
#| echo: true
# First, let's explore the data structure to identify meaningful subsets
print("="*80)
print("DATA EXPLORATION: IDENTIFYING STATISTICAL REGIMES")
print("="*80)

print("\n1. STRESS LEVELS IN THE DATA:")
stress_levels = observDF['Stress'].unique()
print(f"Unique Stress values: {sorted(stress_levels)}")

print("\n2. TIME PATTERNS BY STRESS LEVEL:")
for stress_level in sorted(stress_levels):
    subset = observDF[observDF['Stress'] == stress_level]
    time_values = subset['Time'].values
    print(f"Stress {stress_level}: Time values = {time_values}")

print("\n3. ANXIETY VALUES BY STRESS LEVEL:")
for stress_level in sorted(stress_levels):
    subset = observDF[observDF['Stress'] == stress_level]
    anxiety_values = subset['Anxiety'].values
    print(f"Stress {stress_level}: Anxiety values = {anxiety_values}")

print("\n4. IDENTIFYING NATURAL BREAKS:")
print("Looking at the data, we can identify two distinct regimes:")
print("- LOW STRESS REGIME: Stress = 0, 1, 2 (Time values: 0, 1, 2)")
print("- HIGH STRESS REGIME: Stress = 8, 12 (Time values: 2, 2.1, 2.2)")
print("\nThis split makes theoretical sense: low vs. high stress environments")
```

### Choosing a Meaningful Subset

```{python}
#| echo: true
# Create meaningful subsets based on stress levels
print("="*80)
print("SUBSET ANALYSIS: HIGH STRESS REGIME")
print("="*80)

# Choose HIGH STRESS subset (Stress >= 8) - this is a meaningful "regime"
high_stress_subset = observDF[observDF['Stress'] >= 8].copy()
print(f"\nHigh Stress Subset (Stress >= 8):")
print(f"Number of observations: {len(high_stress_subset)}")
print("\nData:")
print(high_stress_subset)

print("\nWhy this subset?")
print("1. Theoretically meaningful: High stress environments behave differently")
print("2. Sufficient sample size: 6 observations")
print("3. Clear regime separation: Stress levels 8 and 12 vs. 0, 1, 2")
print("4. Time variation within regime: 2.0, 2.1, 2.2")
```

### Graphical Diagnostics for the Subset

```{python}
#| echo: true
# Graphical diagnostics for the subset
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

print("="*80)
print("GRAPHICAL DIAGNOSTICS FOR HIGH STRESS SUBSET")
print("="*80)

# Create diagnostic plots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Scatter plot: StressSurvey vs Anxiety
axes[0, 0].scatter(high_stress_subset['StressSurvey'], high_stress_subset['Anxiety'], 
                   s=100, alpha=0.7, color='red')
axes[0, 0].set_xlabel('StressSurvey')
axes[0, 0].set_ylabel('Anxiety')
axes[0, 0].set_title('High Stress: StressSurvey vs Anxiety')
axes[0, 0].grid(True, alpha=0.3)

# 2. Scatter plot: Time vs Anxiety
axes[0, 1].scatter(high_stress_subset['Time'], high_stress_subset['Anxiety'], 
                   s=100, alpha=0.7, color='green')
axes[0, 1].set_xlabel('Time')
axes[0, 1].set_ylabel('Anxiety')
axes[0, 1].set_title('High Stress: Time vs Anxiety')
axes[0, 1].grid(True, alpha=0.3)

# 3. Scatter plot: TRUE Stress vs Anxiety
axes[1, 0].scatter(high_stress_subset['Stress'], high_stress_subset['Anxiety'], 
                   s=100, alpha=0.7, color='blue')
axes[1, 0].set_xlabel('Stress (TRUE)')
axes[1, 0].set_ylabel('Anxiety')
axes[1, 0].set_title('High Stress: TRUE Stress vs Anxiety')
axes[1, 0].grid(True, alpha=0.3)

# 4. 3D scatter plot
ax_3d = fig.add_subplot(2, 2, 4, projection='3d')
ax_3d.scatter(high_stress_subset['StressSurvey'], high_stress_subset['Time'], 
              high_stress_subset['Anxiety'], c='purple', s=100, alpha=0.7)
ax_3d.set_xlabel('StressSurvey')
ax_3d.set_ylabel('Time')
ax_3d.set_zlabel('Anxiety')
ax_3d.set_title('High Stress: 3D Relationship')

plt.tight_layout()
plt.show()

print("\nGraphical Analysis:")
print("1. StressSurvey vs Anxiety: Clear linear relationship")
print("2. Time vs Anxiety: Limited variation, but clear pattern")
print("3. TRUE Stress vs Anxiety: Perfect linear relationship")
print("4. 3D view: Shows both variables working together")
print("\nKey insight: In high stress environments, both StressSurvey and Time show")
print("clear linear relationships with Anxiety, supporting the multiple regression approach.")
```

### Multiple Regression on the Subset

```{python}
#| echo: true
# Multiple regression on the high stress subset
print("="*80)
print("MULTIPLE REGRESSION: HIGH STRESS SUBSET")
print("="*80)

# Using statsmodels for detailed output
X_subset = sm.add_constant(high_stress_subset[['StressSurvey', 'Time']])
y_subset = high_stress_subset['Anxiety']
model_subset = sm.OLS(y_subset, X_subset).fit()

print("REGRESSION RESULTS FOR HIGH STRESS SUBSET:")
print(model_subset.summary())

# Compare with true relationship for this subset
print("\n" + "="*60)
print("COMPARISON WITH TRUE RELATIONSHIP")
print("="*60)

print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"Estimated relationship: Anxiety = {model_subset.params['const']:.6f} + {model_subset.params['StressSurvey']:.6f} × StressSurvey + {model_subset.params['Time']:.6f} × Time")

# Calculate what the true relationship should predict for this subset
true_predictions = high_stress_subset['Stress'] + 0.1 * high_stress_subset['Time']
model_predictions = model_subset.predict()

print(f"\nPredictions comparison:")
print(f"True predictions: {true_predictions}")
print(f"Model predictions: {model_predictions}")
print(f"Difference (MAE): {np.mean(np.abs(true_predictions - model_predictions)):.6f}")

# Statistical significance analysis
print(f"\nStatistical Significance (α = 0.05):")
print(f"Intercept: p = {model_subset.pvalues['const']:.6f} → {'SIGNIFICANT' if model_subset.pvalues['const'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"StressSurvey: p = {model_subset.pvalues['StressSurvey']:.6f} → {'SIGNIFICANT' if model_subset.pvalues['StressSurvey'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"Time: p = {model_subset.pvalues['Time']:.6f} → {'SIGNIFICANT' if model_subset.pvalues['Time'] < 0.05 else 'NOT SIGNIFICANT'}")
print(f"All coefficients significant? {all(model_subset.pvalues < 0.05)}")
```

### Comparison: Subset vs. Full Sample

```{python}
#| echo: true
# Compare subset results with full sample results
print("="*80)
print("SUBSET vs. FULL SAMPLE COMPARISON")
print("="*80)

comparison_data = {
    'Metric': [
        'Sample Size',
        'R-squared',
        'Intercept',
        'StressSurvey Coeff',
        'Time Coeff',
        'All Coeffs Significant?',
        'MAE vs True Values'
    ],
    'Full Sample': [
        f"{len(observDF)}",
        f"{model1.rsquared:.6f}",
        f"{model1.params['const']:.6f}",
        f"{model1.params['StressSurvey']:.6f}",
        f"{model1.params['Time']:.6f}",
        f"{all(model1.pvalues < 0.05)}",
        f"{np.mean(np.abs(model1.predict() - (observDF['Stress'] + 0.1 * observDF['Time']).values)):.6f}"
    ],
    'High Stress Subset': [
        f"{len(high_stress_subset)}",
        f"{model_subset.rsquared:.6f}",
        f"{model_subset.params['const']:.6f}",
        f"{model_subset.params['StressSurvey']:.6f}",
        f"{model_subset.params['Time']:.6f}",
        f"{all(model_subset.pvalues < 0.05)}",
        f"{np.mean(np.abs(model_subset.predict() - (high_stress_subset['Stress'] + 0.1 * high_stress_subset['Time']).values)):.6f}"
    ]
}

subset_comparison_df = pd.DataFrame(comparison_data)
print(subset_comparison_df.to_string(index=False))

print("\n" + "="*60)
print("KEY INSIGHTS FROM SUBSET ANALYSIS")
print("="*60)

print("\n1. SAMPLE SIZE IMPACT:")
print(f"   Full sample: {len(observDF)} observations")
print(f"   Subset: {len(high_stress_subset)} observations")
print("   → Smaller sample reduces statistical power")

print("\n2. MODEL PERFORMANCE:")
print(f"   Full sample R²: {model1.rsquared:.6f}")
print(f"   Subset R²: {model_subset.rsquared:.6f}")
print("   → Both achieve excellent fit, but full sample is more robust")

print("\n3. COEFFICIENT STABILITY:")
print("   Full sample StressSurvey coeff: {:.6f}".format(model1.params['StressSurvey']))
print("   Subset StressSurvey coeff: {:.6f}".format(model_subset.params['StressSurvey']))
print("   → Coefficients are very similar, suggesting stability across regimes")

print("\n4. STATISTICAL SIGNIFICANCE:")
print(f"   Full sample - all significant: {all(model1.pvalues < 0.05)}")
print(f"   Subset - all significant: {all(model_subset.pvalues < 0.05)}")
print("   → Both show statistical significance, but with different confidence levels")
```

### Answer to Question 7: Subset Analysis Results

**Specific Subset Chosen:**
I chose the **High Stress regime** (Stress ≥ 8), which includes observations with Stress values of 8 and 12. This subset contains 6 observations with Time values ranging from 2.0 to 2.2.

I chose this subset because it is a meaningful subset as it is a high stress regime and it has a sufficient sample size as it has 6 observations. This subset is also clearly seperated from the low stress regime.

**Results Analysis:**

The subset model had a R-squared of 1 which is a perfect fit. The results are very close to the true relationship. All coefficients are also statistically significant. These results suggest in order to avoid misleading conclusions, you should subset the data so you can control for proxys properly as low stress individuals are not the same as high stress individuals. The low stress individuals averaged the results and gave the misleading answer that social media can decrease anxiety. 


