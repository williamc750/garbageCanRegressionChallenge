---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**

## Python Code

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

### Bivariate Regression: Anxiety on StressSurvey

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on StressSurvey
X = observDF['StressSurvey'].values
y = observDF['Anxiety'].values

# Calculate regression coefficients manually
n = len(X)
sum_x = np.sum(X)
sum_y = np.sum(y)
sum_xy = np.sum(X * y)
sum_x2 = np.sum(X * X)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept = (sum_y - slope * sum_x) / n

# Calculate R-squared
y_pred = intercept + slope * X
ss_res = np.sum((y - y_pred) ** 2)
ss_tot = np.sum((y - np.mean(y)) ** 2)
r_squared = 1 - (ss_res / ss_tot)

print("Regression Results:")
print(f"Intercept (beta0): {intercept:.4f}")
print(f"Slope (beta1): {slope:.4f}")
print(f"R-squared: {r_squared:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"Estimated coefficient of StressSurvey: {slope:.4f}")
print(f"Difference from true: {abs(slope - 1.0):.4f}")
```

### What are the estimated coefficients? How do they compare to the true coefficients?

The estimated coefficients are 1.0470 for the StressSurvey (slope) and -1.5240 for the intercept. The true coefficients are 1.0 for the stress, 0 for the intercept, and 0.1 for the Time. The difference from the true coefficients is 0.0470 for StressSurvey and -1.5240 for the intercept. This tells me that the StressSurvey is a good proxy for the true Stress variable, but the intercept is not a good proxy for the true intercept as it is not 0.

### Scatter Plot with Regression Line

```{python}
#| echo: true
# Create scatter plot with regression line
plt.figure(figsize=(10, 6))
plt.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, s=100, color='blue', label='Data Points')

# Plot regression line
x_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred_line = intercept + slope * x_range
plt.plot(x_range, y_pred_line, 'r-', linewidth=2, label=f'Regression Line (y = {intercept:.3f} + {slope:.3f}x)')

plt.xlabel('StressSurvey', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between StressSurvey and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis
print("Fit Analysis:")
print(f"R-squared: {r_squared:.4f} ({r_squared*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res/n):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y - y_pred)):.4f}")

# Check for potential issues
print("\nPotential Issues Analysis:")
print("1. Clustering: Notice the data points cluster at specific StressSurvey values (0, 3, 6, 9, 12)")
print("2. Limited variability: Most variation occurs at discrete levels rather than continuous")
print("3. Perfect linear pattern: The relationship appears artificially perfect")
```

### Commentary on Fit and Potential Issues

**Model Fit Assessment:**

The fit is pretty good with a R-squared of 90.11%. The root mean square error is 0.7828 and the mean absolute error is 0.6241. This tells me that the model is a good fit for the data. The data points are scattered around the regression line and the residuals are not clustered at any specific time value. This is a good sign as it suggests that the model is a good fit for the data.

---

### Bivariate Regression: Anxiety on Time

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on Time
X_time = observDF['Time'].values
y_time = observDF['Anxiety'].values

# Calculate regression coefficients manually
n_time = len(X_time)
sum_x_time = np.sum(X_time)
sum_y_time = np.sum(y_time)
sum_xy_time = np.sum(X_time * y_time)
sum_x2_time = np.sum(X_time * X_time)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope_time = (n_time * sum_xy_time - sum_x_time * sum_y_time) / (n_time * sum_x2_time - sum_x_time * sum_x_time)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept_time = (sum_y_time - slope_time * sum_x_time) / n_time

# Calculate R-squared
y_pred_time = intercept_time + slope_time * X_time
ss_res_time = np.sum((y_time - y_pred_time) ** 2)
ss_tot_time = np.sum((y_time - np.mean(y_time)) ** 2)
r_squared_time = 1 - (ss_res_time / ss_tot_time)

print("Regression Results (Anxiety on Time):")
print(f"Intercept (beta0): {intercept_time:.4f}")
print(f"Slope (beta1): {slope_time:.4f}")
print(f"R-squared: {r_squared_time:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Time: 0.1")
print(f"Estimated coefficient of Time: {slope_time:.4f}")
print(f"Difference from true: {abs(slope_time - 0.1):.4f}")

# Create scatter plot for Time vs Anxiety
plt.figure(figsize=(10, 6))
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', label='Data Points')

# Plot regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_line_time = intercept_time + slope_time * x_range_time
plt.plot(x_range_time, y_pred_line_time, 'r-', linewidth=2, label=f'Regression Line (y = {intercept_time:.3f} + {slope_time:.3f}x)')

plt.xlabel('Time', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between Time and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_time/n_time):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_time - y_pred_time)):.4f}")
```

### What are the estimated coefficients? How do they compare to the true relationship?

The estimated coefficients are 5.3406 for the time and -3.6801 for the intercept. The true coefficients are 0.1 for the time and 0 for the intercept. The difference from the true coefficients is 5.2406 for the time and -3.6801 for the intercept. This tells me that the time is a good proxy for the true time, but the intercept is not a good proxy for the true intercept as it is not 0.

### Scatter Plot with Regression Line: Time vs Anxiety

```{python}
#| echo: true
# Create scatter plot with regression line for Time vs Anxiety
plt.figure(figsize=(10, 6))
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', label='Data Points')

# Plot regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_line_time = intercept_time + slope_time * x_range_time
plt.plot(x_range_time, y_pred_line_time, 'r-', linewidth=2, label=f'Regression Line (y = {intercept_time:.3f} + {slope_time:.3f}x)')

plt.xlabel('Time', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between Time and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis
print("Fit Analysis:")
print(f"R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_time/n_time):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_time - y_pred_time)):.4f}")

# Check for potential issues
print("\nPotential Issues Analysis:")
print("1. Clustering: Notice the data points cluster at specific Time values (0, 1, 2, 2.1, 2.2)")
print("2. Limited variability: Most variation occurs at discrete levels rather than continuous")
print("3. Poor fit: R-squared of 56.3% indicates Time alone is not a strong predictor")
print("4. Large residuals: Substantial prediction errors throughout the range")
```

### Commentary on Fit and Potential Issues

The fit is not very good with a R-squared of 56.3%. The root mean square error is 3.0933 and the mean absolute error is 2.5607. This tells me that the model is not a good fit for the data. The data points are clustered at specific Time values (0, 1, 2, 2.1, 2.2) and the residuals are not random. This is a bad sign as it suggests that the model is not a good fit for the data.


