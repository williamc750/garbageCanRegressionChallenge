---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**


## Python Code

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

### Bivariate Regression: Anxiety on StressSurvey

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on StressSurvey
X = observDF['StressSurvey'].values
y = observDF['Anxiety'].values

# Calculate regression coefficients manually
n = len(X)
sum_x = np.sum(X)
sum_y = np.sum(y)
sum_xy = np.sum(X * y)
sum_x2 = np.sum(X * X)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept = (sum_y - slope * sum_x) / n

# Calculate R-squared
y_pred = intercept + slope * X
ss_res = np.sum((y - y_pred) ** 2)
ss_tot = np.sum((y - np.mean(y)) ** 2)
r_squared = 1 - (ss_res / ss_tot)

print("Regression Results:")
print(f"Intercept (beta0): {intercept:.4f}")
print(f"Slope (beta1): {slope:.4f}")
print(f"R-squared: {r_squared:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"Estimated coefficient of StressSurvey: {slope:.4f}")
print(f"Difference from true: {abs(slope - 1.0):.4f}")
```

###What are the estimated coefficients? How do they compare to the true coefficients?

The estimated coefficients are 1.0470 for the StressSurvey (slope) and -1.5240 for the intercept. The true coefficients are 1.0 for the StressSurvey, 0 for the intercept, and 0.1 for the Time. The difference from the true coefficients is 0.0470 for the StressSurvey and -1.5240 for the intercept. This tells me that the StressSurvey is a good proxy for the true Stress variable, but the intercept is not a good proxy for the true intercept as it is not 0.



### Scatter Plot with Regression Line

```{python}
#| echo: true
# Create scatter plot with regression line
plt.figure(figsize=(10, 6))
plt.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, s=100, color='blue', label='Data Points')

# Plot regression line
x_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred_line = intercept + slope * x_range
plt.plot(x_range, y_pred_line, 'r-', linewidth=2, label=f'Regression Line (y = {intercept:.3f} + {slope:.3f}x)')

plt.xlabel('StressSurvey', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between StressSurvey and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Additional analysis
print("Fit Analysis:")
print(f"R-squared: {r_squared:.4f} ({r_squared*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res/n):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y - y_pred)):.4f}")

# Check for potential issues
print("\nPotential Issues Analysis:")
print("1. Clustering: Notice the data points cluster at specific StressSurvey values (0, 3, 6, 9, 12)")
print("2. Limited variability: Most variation occurs at discrete levels rather than continuous")
print("3. Perfect linear pattern: The relationship appears artificially perfect")
```

### Commentary on Fit and Potential Issues

**Your Analysis:**

The regression line fits the data well as the R-squared is 0.9011. This means that the StressSurvey explains over 90% of the variance in Anxiety. The slope coefficient of 1.0470 is remarkably close to the true relationship coefficient of 1.0, suggesting StressSurvey is an effective proxy for the true Stress variable. However, the negative intercept (-1.5240) is also problematic from a practical standpoint, as it suggests negative anxiety levels when StressSurvey equals zero, which doesn't make psychological sense. This indicates the linear model may not be appropriate for the full range of possible values.

*Please provide your commentary on the regression fit and any potential issues you observe in the data and model. Consider:*

- *How well does the regression line fit the data?*
- *What patterns do you notice in the scatter plot?*
- *Are there any concerns about the data quality or model assumptions?*
- *How might this analysis be improved?*

---

**Example Commentary:**

The regression analysis reveals several important findings and concerns about this dataset. First, the statistical fit is excellent with an R-squared of 0.9011, meaning StressSurvey explains over 90% of the variance in Anxiety. The slope coefficient of 1.0470 is remarkably close to the true relationship coefficient of 1.0, suggesting StressSurvey is an effective proxy for the true Stress variable.

However, several red flags suggest this data may be artificially generated rather than representing real psychological measurements. The most concerning pattern is the perfect clustering of data points at discrete StressSurvey values (0, 3, 6, 9, 12). In real-world psychological research, we would expect continuous variation and more scatter around the regression line due to individual differences, measurement error, and other confounding factors.

The negative intercept (-1.5240) is also problematic from a practical standpoint, as it suggests negative anxiety levels when StressSurvey equals zero, which doesn't make psychological sense. This indicates the linear model may not be appropriate for the full range of possible values.

The perfect linear relationship and lack of residual variation suggest this dataset was likely created for educational purposes rather than collected from actual research. While it effectively demonstrates regression concepts, it doesn't reflect the complexity and noise typically found in real psychological data.

For improvement, I would recommend collecting more diverse data points, including intermediate StressSurvey values, and ensuring the relationship shows realistic scatter patterns that account for individual differences and measurement error.

*[Replace this example with your own commentary]*

---

**Key Observations to Consider:**

1. **Data Clustering**: The data points cluster at specific StressSurvey values (0, 3, 6, 9, 12) rather than showing continuous variation
2. **Perfect Linearity**: The relationship appears artificially perfect for real psychological data
3. **Limited Sample Size**: Only 15 observations may limit generalizability
4. **High R-squared**: While 90.1% variance explained is excellent, it may indicate overfitting or artificial data
5. **Discrete Levels**: Real psychological measurements typically show more natural variation

**Questions for Reflection:**
- Does this data look realistic for psychological measurements?
- What assumptions of linear regression might be violated?
- How would you interpret the negative intercept in practical terms?
- What additional variables might be important to include?



### Bivariate Regression: Anxiety on Time

```{python}
#| echo: true
# Manual bivariate regression of Anxiety on Time
X_time = observDF['Time'].values
y_time = observDF['Anxiety'].values

# Calculate regression coefficients manually
n_time = len(X_time)
sum_x_time = np.sum(X_time)
sum_y_time = np.sum(y_time)
sum_xy_time = np.sum(X_time * y_time)
sum_x2_time = np.sum(X_time * X_time)

# Slope (beta1) = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)
slope_time = (n_time * sum_xy_time - sum_x_time * sum_y_time) / (n_time * sum_x2_time - sum_x_time * sum_x_time)

# Intercept (beta0) = (sum_y - beta1*sum_x) / n
intercept_time = (sum_y_time - slope_time * sum_x_time) / n_time

# Calculate R-squared
y_pred_time = intercept_time + slope_time * X_time
ss_res_time = np.sum((y_time - y_pred_time) ** 2)
ss_tot_time = np.sum((y_time - np.mean(y_time)) ** 2)
r_squared_time = 1 - (ss_res_time / ss_tot_time)

print("Regression Results (Anxiety on Time):")
print(f"Intercept (beta0): {intercept_time:.4f}")
print(f"Slope (beta1): {slope_time:.4f}")
print(f"R-squared: {r_squared_time:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Time: 0.1")
print(f"Estimated coefficient of Time: {slope_time:.4f}")
print(f"Difference from true: {abs(slope_time - 0.1):.4f}")

# Create scatter plot for Time vs Anxiety
plt.figure(figsize=(10, 6))
plt.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', label='Data Points')

# Plot regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_line_time = intercept_time + slope_time * x_range_time
plt.plot(x_range_time, y_pred_line_time, 'r-', linewidth=2, label=f'Regression Line (y = {intercept_time:.3f} + {slope_time:.3f}x)')

plt.xlabel('Time', fontsize=12)
plt.ylabel('Anxiety', fontsize=12)
plt.title('Relationship between Time and Anxiety', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_time/n_time):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_time - y_pred_time)):.4f}")
```

### Multiple Regression: Anxiety on StressSurvey and Time

```{python}
#| echo: true
# Multiple regression of Anxiety on both StressSurvey and Time
# Using matrix algebra for multiple regression: β = (X'X)^(-1)X'y

# Create design matrix X with intercept, StressSurvey, and Time
X_multi = np.column_stack([
    np.ones(len(observDF)),  # Intercept column
    observDF['StressSurvey'].values,  # StressSurvey column
    observDF['Time'].values  # Time column
])

y_multi = observDF['Anxiety'].values

# Calculate coefficients using normal equation: β = (X'X)^(-1)X'y
X_transpose = X_multi.T
X_transpose_X = np.dot(X_transpose, X_multi)
X_transpose_X_inv = np.linalg.inv(X_transpose_X)
X_transpose_y = np.dot(X_transpose, y_multi)
coefficients = np.dot(X_transpose_X_inv, X_transpose_y)

# Extract coefficients
intercept_multi = coefficients[0]
slope_stress_survey = coefficients[1]
slope_time = coefficients[2]

# Calculate predictions and R-squared
y_pred_multi = np.dot(X_multi, coefficients)
ss_res_multi = np.sum((y_multi - y_pred_multi) ** 2)
ss_tot_multi = np.sum((y_multi - np.mean(y_multi)) ** 2)
r_squared_multi = 1 - (ss_res_multi / ss_tot_multi)

print("Multiple Regression Results:")
print(f"Intercept (β₀): {intercept_multi:.4f}")
print(f"StressSurvey coefficient (β₁): {slope_stress_survey:.4f}")
print(f"Time coefficient (β₂): {slope_time:.4f}")
print(f"R-squared: {r_squared_multi:.4f}")

# Compare to true relationship
print()
print("Comparison to True Relationship:")
print("True relationship: Anxiety = Stress + 0.1 × Time")
print(f"True coefficient of Stress: 1.0")
print(f"True coefficient of Time: 0.1")
print()
print(f"Estimated StressSurvey coefficient: {slope_stress_survey:.4f}")
print(f"Difference from true Stress: {abs(slope_stress_survey - 1.0):.4f}")
print()
print(f"Estimated Time coefficient: {slope_time:.4f}")
print(f"Difference from true Time: {abs(slope_time - 0.1):.4f}")

# Additional fit statistics
print(f"\nFit Analysis:")
print(f"R-squared: {r_squared_multi:.4f} ({r_squared_multi*100:.1f}% of variance explained)")
print(f"Root Mean Square Error: {np.sqrt(ss_res_multi/len(y_multi)):.4f}")
print(f"Mean Absolute Error: {np.mean(np.abs(y_multi - y_pred_multi)):.4f}")

# Create 3D scatter plot
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Time'], observDF['Anxiety'], 
           c='blue', alpha=0.7, s=100, label='Data Points')

# Create mesh for regression plane
stress_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 10)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 10)
Stress_mesh, Time_mesh = np.meshgrid(stress_range, time_range)
Anxiety_pred = intercept_multi + slope_stress_survey * Stress_mesh + slope_time * Time_mesh

# Plot regression plane
ax.plot_surface(Stress_mesh, Time_mesh, Anxiety_pred, alpha=0.3, color='red', 
                label='Regression Plane')

ax.set_xlabel('StressSurvey')
ax.set_ylabel('Time')
ax.set_zlabel('Anxiety')
ax.set_title('Multiple Regression: Anxiety = f(StressSurvey, Time)')
plt.tight_layout()
plt.show()
```

